# YouTube Comment Sentiment Analysis ğŸ¥ğŸ“Š

A comprehensive machine learning project that analyzes the sentiment of YouTube comments using Natural Language Processing (NLP) techniques. The project includes a Flask web application for real-time sentiment analysis and visualization, complete with MLflow experiment tracking and Docker deployment capabilities.

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![Flask](https://img.shields.io/badge/flask-v2.0+-green.svg)
![MLflow](https://img.shields.io/badge/mlflow-v2.0+-orange.svg)
![Docker](https://img.shields.io/badge/docker-supported-blue.svg)

## ğŸš€ Features

- **Sentiment Analysis**: Classify YouTube comments as positive, negative, or neutral
- **Real-time Processing**: Flask API for live sentiment analysis
- **Data Visualization**: Generate word clouds and sentiment distribution charts
- **ML Pipeline**: Complete data preprocessing, model training, and evaluation pipeline
- **Experiment Tracking**: MLflow integration for model versioning and tracking
- **Containerization**: Docker support for easy deployment
- **Model Performance**: Confusion matrix and performance metrics visualization
- **DVC Integration**: Data Version Control for reproducible data science workflows

## ğŸ› ï¸ Tech Stack

- **Backend**: Python, Flask, MLflow
- **Machine Learning**: scikit-learn, NLTK, pandas, numpy
- **Visualization**: matplotlib, wordcloud
- **Containerization**: Docker
- **Data Management**: DVC (Data Version Control)
- **Environment**: Virtual environment with comprehensive dependencies

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+
- pip
- Docker (optional, for containerized deployment)

### Setup

1. **Clone the repository**
```bash
git clone git@github.com:Pavanachowdary/youtube-comment-sentiment-analysis.git
cd youtube-comment-sentiment-analysis
```

2. **Create and activate virtual environment**
```bash
python -m venv myenv
source myenv/bin/activate  # On Windows: myenv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Install the package in development mode**
```bash
pip install -e .
```

## ğŸš€ Quick Start

### Running the Flask Application

```bash
cd flask_app
python app.py
```

The application will be available at `http://localhost:5000`

### API Endpoints

- `POST /predict` - Analyze sentiment of a single comment
- `GET /health` - Health check endpoint
- `POST /batch_predict` - Analyze multiple comments at once
- `GET /wordcloud` - Generate word cloud visualization

### Example API Usage

```python
import requests

# Single comment prediction
response = requests.post('http://localhost:5000/predict', 
                        json={'comment': 'This video is amazing!'})
print(response.json())
```

## ğŸ§ª Model Training

### Training Pipeline

1. **Data Preprocessing**
```bash
python src/data/make_dataset.py
```

2. **Feature Engineering**
```bash
python src/features/build_features.py
```

3. **Model Training**
```bash
python src/models/train_model.py
```

4. **Model Evaluation**
```bash
python src/models/predict_model.py
```

### MLflow Tracking

Start MLflow UI to track experiments:
```bash
mlflow ui
```

Access the MLflow dashboard at `http://localhost:5000` to view experiment results, model metrics, and artifacts.

## ğŸ³ Docker Deployment

### Build and Run with Docker

```bash
# Build the Docker image
docker build -t yt-sentiment-analysis .

# Run the container
docker run -p 5000:5000 yt-sentiment-analysis
```

### Using Docker Compose (if available)

```bash
docker-compose up
```

## ğŸ“Š Data Pipeline

The project follows a structured data science workflow:

1. **Raw Data**: YouTube comments dataset
2. **Data Preprocessing**: Text cleaning, tokenization, stopword removal
3. **Feature Engineering**: TF-IDF vectorization, sentiment labeling
4. **Model Training**: Multiple algorithms comparison (Logistic Regression, SVM, etc.)
5. **Model Evaluation**: Performance metrics and confusion matrix
6. **Model Deployment**: Flask API with trained model

## ğŸ“ Project Structure

```
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile              <- Makefile with commands like `make data` or `make train`
â”œâ”€â”€ README.md             <- The top-level README for developers using this project
â”œâ”€â”€ requirements.txt      <- The requirements file for reproducing the analysis environment
â”œâ”€â”€ setup.py             <- Makes project pip installable (pip install -e .)
â”œâ”€â”€ Dockerfile           <- Docker configuration for containerized deployment
â”œâ”€â”€ dvc.yaml             <- DVC pipeline configuration
â”œâ”€â”€ params.yaml          <- Parameters for model training and evaluation
â”œâ”€â”€ appspec.yml          <- AWS CodeDeploy application specification
â”œâ”€â”€ tox.ini              <- Tox file with settings for running tox
â”‚
â”œâ”€â”€ data/                <- Data directory (managed by DVC)
â”‚   â”œâ”€â”€ external         <- Data from third party sources
â”‚   â”œâ”€â”€ interim          <- Intermediate data that has been transformed
â”‚   â”œâ”€â”€ processed        <- The final, canonical data sets for modeling
â”‚   â””â”€â”€ raw              <- The original, immutable data dump
â”‚
â”œâ”€â”€ docs/                <- Sphinx documentation
â”‚   â”œâ”€â”€ commands.rst
â”‚   â”œâ”€â”€ conf.py
â”‚   â”œâ”€â”€ getting-started.rst
â”‚   â””â”€â”€ index.rst
â”‚
â”œâ”€â”€ flask_app/           <- Flask web application
â”‚   â”œâ”€â”€ app.py           <- Main Flask application with API endpoints
â”‚   â””â”€â”€ requirements.txt <- Flask app specific requirements
â”‚
â”œâ”€â”€ models/              <- Trained and serialized models, model predictions, summaries
â”‚
â”œâ”€â”€ notebooks/           <- Jupyter notebooks for data exploration and analysis
â”‚
â”œâ”€â”€ references/          <- Data dictionaries, manuals, and explanatory materials
â”‚
â”œâ”€â”€ reports/             <- Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures/         <- Generated graphics and figures to be used in reporting
â”‚
â”œâ”€â”€ scripts/             <- Utility scripts
â”‚   â”œâ”€â”€ mlflow_test.py
â”‚   â”œâ”€â”€ promote_model.py
â”‚   â”œâ”€â”€ test_flask_api.py
â”‚   â”œâ”€â”€ test_load_model.py
â”‚   â”œâ”€â”€ test_model_performance.py
â”‚   â””â”€â”€ test_model_signature.py
â”‚
â””â”€â”€ src/                 <- Source code for use in this project
    â”œâ”€â”€ __init__.py      <- Makes src a Python module
    â”œâ”€â”€ data/            <- Scripts to download or generate data
    â”œâ”€â”€ features/        <- Scripts to turn raw data into features for modeling
    â”œâ”€â”€ models/          <- Scripts to train models and make predictions
    â””â”€â”€ visualization/   <- Scripts to create exploratory and results oriented visualizations
```

## ğŸ§ª Testing

Run tests to ensure everything is working correctly:

```bash
# Test environment setup
python test_environment.py

# Test Flask API
python scripts/test_flask_api.py

# Test model loading and performance
python scripts/test_load_model.py
python scripts/test_model_performance.py
```

## ğŸ“ˆ Model Performance

The project includes comprehensive model evaluation:

- **Accuracy Metrics**: Precision, Recall, F1-Score
- **Confusion Matrix**: Visual representation of model performance
- **Cross-validation**: Robust model validation
- **Feature Importance**: Understanding key sentiment indicators

## ğŸ”§ Configuration

Key configuration files:

- `params.yaml`: Model parameters and hyperparameters
- `dvc.yaml`: Data pipeline configuration
- `requirements.txt`: Python dependencies
- `Dockerfile`: Container configuration

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Pavana Nettem**

## ğŸ™ Acknowledgments

- Built using the [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/) project template
- Thanks to the open-source community for the amazing tools and libraries
- YouTube API for data access capabilities

## ğŸš€ Deployment

The project includes configuration for various deployment platforms:

- **AWS CodeDeploy**: Using `appspec.yml`
- **Docker**: Containerized deployment
- **Local Development**: Flask development server

## ğŸ”® Future Enhancements

- [ ] Real-time YouTube comment streaming
- [ ] Multi-language sentiment analysis
- [ ] Chrome extension integration
- [ ] Advanced visualization dashboard
- [ ] Model performance monitoring
- [ ] A/B testing framework for model comparison

---

â­ **Star this repository if you found it helpful!**
